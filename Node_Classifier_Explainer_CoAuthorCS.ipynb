{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingGuzman/Node-Classifier-Explainer/blob/main/Node_Classifier_Explainer_CoAuthorCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run all the cells successively"
      ],
      "metadata": {
        "id": "46OzpmAQx5WY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o-N2dk6oGMn",
        "outputId": "60b16893-6dd8-4913-97ae-b191f9b02913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 6.6 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlFlxfL5dgn2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import torch_geometric as torch_geometric\n",
        "import math\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import GCNConv,GINConv\n",
        "from torch.distributions import Bernoulli,Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vBo75xW9_o"
      },
      "source": [
        "Below train mask is for \n",
        "CoAuthor dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0jumoCh6f5r"
      },
      "outputs": [],
      "source": [
        "train_mask=random.sample(range(0, 15000), 15000)\n",
        "test_mask=random.sample(range(15001,18333),3332)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS_GFgpUrW9t",
        "outputId": "732f8dbd-1e9f-4d5b-973a-b7b457d73083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333])\n",
            "tensor([13,  2, 13,  ...,  0,  8,  5])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset1=torch_geometric.datasets.Coauthor(root='/tmp/Coauthor',name=\"CS\")\n",
        "num_node_features=dataset1.num_node_features\n",
        "dir(dataset1)\n",
        "#print(dataset.url)\n",
        "\n",
        "print(dataset1.num_classes)\n",
        "print(dataset1.data)\n",
        "dat=dataset1.data\n",
        "print(dat.y)\n",
        "dataset=[]\n",
        "dataset.append(dat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U7VyPN8TwQM",
        "outputId": "e432c2ac-cbaa-4152-981c-9d65119cf287"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__orig_bases__',\n",
              " '__parameters__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_data_list',\n",
              " '_download',\n",
              " '_indices',\n",
              " '_process',\n",
              " 'collate',\n",
              " 'copy',\n",
              " 'data',\n",
              " 'download',\n",
              " 'get',\n",
              " 'index_select',\n",
              " 'indices',\n",
              " 'len',\n",
              " 'name',\n",
              " 'num_classes',\n",
              " 'num_edge_features',\n",
              " 'num_features',\n",
              " 'num_node_features',\n",
              " 'pre_filter',\n",
              " 'pre_transform',\n",
              " 'process',\n",
              " 'processed_dir',\n",
              " 'processed_file_names',\n",
              " 'processed_paths',\n",
              " 'raw_dir',\n",
              " 'raw_file_names',\n",
              " 'raw_paths',\n",
              " 'root',\n",
              " 'shuffle',\n",
              " 'slices',\n",
              " 'transform',\n",
              " 'url']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dir(dataset1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nN_m_krJh7V"
      },
      "outputs": [],
      "source": [
        "class GNNStack(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, task='node'):\n",
        "        super(GNNStack, self).__init__()\n",
        "        self.task = task\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
        "        self.lns = nn.ModuleList()\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        for l in range(3):\n",
        "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
        "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "        if not (self.task == 'node' or self.task == 'graph'):\n",
        "            raise RuntimeError('Unknown task.')\n",
        "\n",
        "        self.dropout = 0.25\n",
        "        self.num_layers = 3\n",
        "\n",
        "    def build_conv_model(self, input_dim, hidden_dim):\n",
        "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
        "        if self.task == 'node':\n",
        "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
        "        else:\n",
        "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        if data.num_node_features == 0:\n",
        "          x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            emb = x\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            if not i == self.num_layers - 1:\n",
        "                x = self.lns[i](x)\n",
        "\n",
        "        if self.task == 'graph':\n",
        "            x = pyg_nn.global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "        reward=F.softmax(x,dim=1)\n",
        "        \n",
        "\n",
        "        return emb,reward,F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)\n",
        "        #F.nll_loss(pred, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLg7Ab_QEa1J"
      },
      "outputs": [],
      "source": [
        "def train(dataset, task):\n",
        "    torch.manual_seed(2)\n",
        "    if task == 'graph':\n",
        "        data_size = len(dataset)\n",
        "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
        "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
        "    else:\n",
        "         loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(max(num_node_features, 1), 64, 15, task=task)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model=model.to(device)\n",
        "    \n",
        "    # train\n",
        "    for epoch in range(250):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            #print(batch.train_mask, '----')\n",
        "            opt.zero_grad()\n",
        "            batch=batch.to(device)\n",
        "            embedding,reward,pred = model(batch)\n",
        "            \n",
        "            label = batch.y\n",
        "            #label=label.float()\n",
        "            #labelonehot=torch.nn.functional.one_hot(label,num_classes=2)\n",
        "            #labelonehot=labelonehot.float()\n",
        "            #labelonehot=torch.zeros_like(label)\n",
        "            #labelonehot=labelonehot.scatter(1,label, 1)\n",
        "            #pred = pred[batch.train_mask]\n",
        "            #label = label[batch.train_mask]\n",
        "            \n",
        "            loss = model.loss(pred[train_mask], label[train_mask])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        #writer.add_scalar(\"loss\", total_loss, epoch)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "          print(epoch)\n",
        "          #print(reward)\n",
        "          print(loss*100)\n",
        "          #print(pred.shape)\n",
        "            #test_acc = test(test_loader, model)\n",
        "            #print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
        "                #epoch, total_loss, test_acc))\n",
        "            #writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzBLmgZOcTB_"
      },
      "outputs": [],
      "source": [
        "def test(loader, model, task='node'):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total=0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            emb, reward,pred = model(data)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            label = data.y\n",
        "            \n",
        "\n",
        "        #if task == 'node':\n",
        "            #mask = data.val_mask if is_validation else data.test_mask\n",
        "            # node classification: only evaluate on nodes in test set\n",
        "            #pred = pred[mask]\n",
        "            #label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "        total+=len(label)\n",
        "    \n",
        "    #if task == 'graph':\n",
        "     #   total = len(loader.dataset) \n",
        "    #else:\n",
        "        #total = len(loader.dataset)*50\n",
        "        \n",
        "    return correct / total,total,correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ermBwrR6ca4Z",
        "outputId": "40ad54cd-fa6d-40b0-b0f9-e7df06e92cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor(274.9610, grad_fn=<MulBackward0>)\n",
            "20\n",
            "tensor(31.1824, grad_fn=<MulBackward0>)\n",
            "40\n",
            "tensor(20.1074, grad_fn=<MulBackward0>)\n",
            "60\n",
            "tensor(14.5432, grad_fn=<MulBackward0>)\n",
            "80\n",
            "tensor(10.6966, grad_fn=<MulBackward0>)\n",
            "100\n",
            "tensor(8.4732, grad_fn=<MulBackward0>)\n",
            "120\n",
            "tensor(6.6871, grad_fn=<MulBackward0>)\n",
            "140\n",
            "tensor(5.6515, grad_fn=<MulBackward0>)\n",
            "160\n",
            "tensor(5.1964, grad_fn=<MulBackward0>)\n",
            "180\n",
            "tensor(3.9418, grad_fn=<MulBackward0>)\n",
            "200\n",
            "tensor(3.7084, grad_fn=<MulBackward0>)\n",
            "220\n",
            "tensor(3.1103, grad_fn=<MulBackward0>)\n",
            "240\n",
            "tensor(3.0385, grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "task = 'node'\n",
        "\n",
        "model = train(dataset, task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k40NQJ8GdFl",
        "outputId": "2dfb6f0f-1e9e-401f-9737-cd8527b63dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9150660264105642\n",
            "3332\n"
          ]
        }
      ],
      "source": [
        "embedding,reward,pred = model(dat)\n",
        "pred=pred.argmax(dim=1)\n",
        "label=dat.y\n",
        "correct=pred[test_mask].eq(label[test_mask]).sum().item()\n",
        "total=len(label[test_mask])\n",
        "accuracy=correct/total\n",
        "print(accuracy)\n",
        "print(total)\n",
        "#print(len(pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Q2KDu0G4nf"
      },
      "outputs": [],
      "source": [
        "class GCNPolicy(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GCNPolicy, self).__init__()\n",
        "    self.conv1=GCNConv(num_gennode_features,num_hidden_features)\n",
        "    self.conv2=GCNConv(num_hidden_features,num_hidden_features)\n",
        "    self.conv3=GCNConv(num_hidden_features,num_gennode_features)\n",
        "    self.lin1=nn.Sequential(nn.Linear((num_gennodes)*num_gennode_features, num_hidden_features),nn.ReLU(),nn.Linear(num_hidden_features,num_hidden_features),nn.ReLU(),nn.Linear(num_hidden_features,num_gennodes-1))\n",
        "  def forward(self,gendata):\n",
        "        x, edge_index = gendata.x, gendata.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x=F.relu(x)\n",
        "        x=self.conv3(x,edge_index)\n",
        "        x=F.relu(x)\n",
        "        x=torch.flatten(x)\n",
        "        x=self.lin1(x)\n",
        "        x=F.softmax(x/temp)\n",
        "        #x=Bernoulli(x)\n",
        "        #action=x.sample()\n",
        "       \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Node indices used for graphs in the paper//\n",
        "1) Graph 1 :- 320,420  p=0.954 class label=0//\n",
        "2) Graph 2 :- 1234,30 p=1.00 class label=11"
      ],
      "metadata": {
        "id": "8cAokMXZhXCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsesmkWi_xk_"
      },
      "outputs": [],
      "source": [
        "keepnodes=torch.tensor(5)\n",
        "subgraphy=torch.tensor(5)\n",
        "subgraphx=torch.tensor(5)\n",
        "newfeatures=torch.tensor(5)\n",
        "newy=torch.tensor(5)\n",
        "count=0\n",
        "count1=0\n",
        "nodeid=[1234,30] # put the corresponding node ids for graph 1 and graph2 to generate the diagrams in the paper for the CoAuthor dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxPzRs4utxgu"
      },
      "source": [
        "new code!!!16/04/22"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(nodeid)):\n",
        "  subgraphind=[]\n",
        "  edgeind=dat.edge_index\n",
        "\n",
        "  subgraph=pyg_utils.k_hop_subgraph(nodeid[i],1,edgeind,relabel_nodes=False)\n",
        "  subgraph1=pyg_utils.k_hop_subgraph(nodeid[i],1,edgeind,relabel_nodes=True)\n",
        "  if count==0:\n",
        "    keepnodes=subgraph[0]\n",
        "    count=1\n",
        "  else:\n",
        "\n",
        "    keepnodes=torch.cat([keepnodes,subgraph[0]],dim=0)\n",
        "  print(subgraph[0])\n",
        "\n",
        "#print(len(subgraph[0]))\n",
        "\n",
        "  print(subgraph[2])\n",
        "  x=dat.x\n",
        "  y=dat.y\n",
        "  #print(y)\n",
        "  features=[x[i] for i in subgraph[0]]\n",
        "  features=torch.stack(features)\n",
        "  #print(subgraphx)\n",
        "  subgraphind=subgraph1[1]\n",
        "  print(subgraphind)\n",
        "  k=[y[i] for i in subgraph[0]]\n",
        "  k=torch.stack(k)\n",
        "  if count1==0:\n",
        "    subgraphy=k\n",
        "    subgraphx=features\n",
        "    count1=1\n",
        "  else:\n",
        "    subgraphy=torch.cat([subgraphy,k],dim=0)\n",
        "    subgraphx=torch.cat([subgraphx,features],dim=0)\n",
        "    #lis=list(set(list(flatten(L))))\n",
        "  print(subgraphx.shape)\n",
        "  print(subgraphy)\n",
        "  nodelabels=subgraphy\n",
        "  #print(len(subgraphy))\n",
        "  newdata=Data(x=subgraphx,edge_index=subgraphind,y=subgraphy)\n",
        "  G=pyg_utils.to_networkx(newdata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOb7RZgkh2KG",
        "outputId": "c89cdd01-50ba-4e74-b700-f57b416e13c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1234, 16563])\n",
            "tensor([0])\n",
            "tensor([[0, 1],\n",
            "        [1, 0]])\n",
            "torch.Size([2, 6805])\n",
            "tensor([11, 11])\n",
            "tensor([   30,  2399,  2475,  3872,  3882,  6842,  7133,  7362, 12823, 14308,\n",
            "        15636, 16145])\n",
            "tensor([0])\n",
            "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,\n",
            "          2,  3,  3,  3,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  8,\n",
            "          8,  8,  8,  9,  9, 10, 10, 10, 11, 11],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  0,  2,  4,  5,  0,  1,  4,\n",
            "          5,  0,  4, 11,  0,  1,  2,  3,  5,  0,  1,  2,  4,  0,  8, 10,  0,  0,\n",
            "          6,  9, 10,  0,  8,  0,  6,  8,  0,  3]])\n",
            "torch.Size([14, 6805])\n",
            "tensor([11, 11, 12, 12, 12,  1, 12, 12, 12,  1, 12,  1, 12,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jMoHUMjtxQT",
        "outputId": "b299b3af-cded-457d-91ec-737741c3b2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  2,  3,  3,\n",
            "          3,  3,  4,  4,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  8,\n",
            "          9,  9,  9,  9, 10, 10, 11, 11, 11, 12, 12, 13],\n",
            "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  3,  5,  6,  0,  2,\n",
            "          5,  6,  0,  5, 12,  0,  2,  3,  4,  6,  0,  2,  3,  5,  0,  9, 11,  0,\n",
            "          0,  7, 10, 11,  0,  9,  0,  7,  9,  0,  4,  1]])\n"
          ]
        }
      ],
      "source": [
        "sub=pyg_utils.subgraph(keepnodes,edgeind,relabel_nodes=True)\n",
        "print(sub[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQAP0Eu5Ruw",
        "outputId": "a610080f-80d6-4731-eabb-81b681228a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   30,  1234,  2399,  2475,  3872,  3882,  6842,  7133,  7362, 12823,\n",
            "        14308, 15636, 16145, 16563])\n",
            "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  2,  3,  3,\n",
            "          3,  3,  4,  4,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  8,\n",
            "          9,  9,  9,  9, 10, 10, 11, 11, 11, 12, 12, 13],\n",
            "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  3,  5,  6,  0,  2,\n",
            "          5,  6,  0,  5, 12,  0,  2,  3,  4,  6,  0,  2,  3,  5,  0,  9, 11,  0,\n",
            "          0,  7, 10, 11,  0,  9,  0,  7,  9,  0,  4,  1]])\n"
          ]
        }
      ],
      "source": [
        "#keepnodes=torch.cat(keepnodes,dim=1)\n",
        "v=torch.sort(keepnodes)\n",
        "newkeep=v[0]\n",
        "x=dat.x\n",
        "y=dat.y\n",
        "  #print(y)\n",
        "newfeatures=[x[i] for i in newkeep]\n",
        "newfeatures=torch.stack(newfeatures)\n",
        "newy=[y[i] for i in newkeep]\n",
        "newy=torch.stack(newy)\n",
        "\n",
        "\n",
        "print(v[0])\n",
        "subgraphedge=pyg_utils.subgraph(keepnodes,edgeind,relabel_nodes=True)\n",
        "print(subgraphedge[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This segment generates the merged graph after sampling"
      ],
      "metadata": {
        "id": "qllo55PzbcVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "TJlc9EUqOKtu",
        "outputId": "e527b3f6-a0fc-4e8f-9557-2d9a52d3bc46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12, 11, 12, 12,  1, 12, 12, 12,  1, 12,  1, 12,  1, 11])\n",
            "tensor([12, 11, 12, 12,  1, 12, 12, 12,  1, 12,  1, 12,  1, 11])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "classid=[]\n",
        "subgraphdata=Data(x=newfeatures,edge_index=subgraphedge[0],y=newy)\n",
        "H=pyg_utils.to_networkx(subgraphdata,to_undirected=True)\n",
        "print(subgraphdata.y)\n",
        "\n",
        "nodelabels=subgraphdata.y\n",
        "print(nodelabels)\n",
        "classid.append(nodelabels[-1])\n",
        "#plt.figure(1,figsize=(5,3))\n",
        "nx.draw_networkx(H,node_size=120,node_color=nodelabels, with_labels=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxEy8e0-atVI",
        "outputId": "e0a22927-9b58-4b32-f4a7-3b9292b6e9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12, 11, 12, 12,  1, 12, 12, 12,  8, 12, 12, 12,  1, 11])\n",
            "tensor([12, 11, 12, 12,  1, 12, 12, 12,  1, 12,  1, 12,  1, 11])\n"
          ]
        }
      ],
      "source": [
        "emb,rew,pred=model(subgraphdata)\n",
        "#print(pred)\n",
        "print(pred.argmax(dim=1))\n",
        "print(subgraphdata.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMZJE9joqztD"
      },
      "source": [
        "Now, we construct the deleted subgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghc-UQwbrxbA",
        "outputId": "d5fd9bc0-8b07-4523-80dd-c4766634bf0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "print(len(subgraphdata.y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ip7ByYdbY3B",
        "outputId": "96eb2d35-6ff3-442d-edbc-024612669617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "tensor([   30,  1234,  2399,  2475,  3872,  3882,  6842,  7133,  7362, 12823,\n",
            "        14308, 15636, 16145, 16563])\n",
            "tensor([   30,  1234,  2399,  2475,  3872,  3882,  6842,  7133,  7362, 12823,\n",
            "        14308, 15636, 16145])\n",
            "tensor([[   30,    30,    30,    30,    30,    30,    30,    30,    30,    30,\n",
            "            30,  2399,  2399,  2399,  2399,  2475,  2475,  2475,  2475,  3872,\n",
            "          3872,  3872,  3882,  3882,  3882,  3882,  3882,  6842,  6842,  6842,\n",
            "          6842,  7133,  7133,  7133,  7362, 12823, 12823, 12823, 12823, 14308,\n",
            "         14308, 15636, 15636, 15636, 16145, 16145],\n",
            "        [ 2399,  2475,  3872,  3882,  6842,  7133,  7362, 12823, 14308, 15636,\n",
            "         16145,    30,  2475,  3882,  6842,    30,  2399,  3882,  6842,    30,\n",
            "          3882, 16145,    30,  2399,  2475,  3872,  6842,    30,  2399,  2475,\n",
            "          3882,    30, 12823, 15636,    30,    30,  7133, 14308, 15636,    30,\n",
            "         12823,    30,  7133, 12823,    30,  3872]])\n",
            "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  3,  3,  3,\n",
            "          3,  4,  4,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  8,  9,\n",
            "          9,  9,  9, 10, 10, 11, 11, 11, 12, 12],\n",
            "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,  0,  3,  5,  6,  0,  2,  5,\n",
            "          6,  0,  5, 12,  0,  2,  3,  4,  6,  0,  2,  3,  5,  0,  9, 11,  0,  0,\n",
            "          7, 10, 11,  0,  9,  0,  7,  9,  0,  4]])\n"
          ]
        }
      ],
      "source": [
        "deletedsubgraphnodes=newkeep[0:len(newkeep)-1]\n",
        "print(len(newkeep))\n",
        "print(newkeep)\n",
        "print(deletedsubgraphnodes)\n",
        "deletedsubgraphind1=pyg_utils.subgraph(deletedsubgraphnodes,dat.edge_index)\n",
        "print(deletedsubgraphind1[0])\n",
        "deletedsubgraphind2=pyg_utils.subgraph(deletedsubgraphnodes,deletedsubgraphind1[0],relabel_nodes=True)\n",
        "print(deletedsubgraphind2[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following segment generates the query node and template graph after edge deletion from the merged graph"
      ],
      "metadata": {
        "id": "tNd-pM7PbqeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "yaneDpSnbeeG",
        "outputId": "c226b585-d8e6-4bec-8eb7-14667c7e4861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "tensor([12, 11, 12, 12,  1, 12, 12, 12,  1, 12,  1, 12,  1, 11])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(len(subgraphx))\n",
        "print(subgraphdata.y)\n",
        "deletedsubgraphdata=Data(x=newfeatures,edge_index=deletedsubgraphind2[0],y=newy)\n",
        "L=pyg_utils.to_networkx(deletedsubgraphdata,to_undirected=True)\n",
        "nodelabels=deletedsubgraphdata.y\n",
        "#e=[(2,1),(1,2)]\n",
        "#L.add_edges_from(e)\n",
        "#L.add_edge(1,2)\n",
        "\n",
        "nx.draw_networkx(L,node_size=150,node_color=nodelabels,with_labels=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6di5oZcFre0k"
      },
      "outputs": [],
      "source": [
        "def takeFirst(elem):\n",
        "  return elem[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Neighborhood generator"
      ],
      "metadata": {
        "id": "lyUJ2XDgcTVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VGlh91dqRLAm",
        "outputId": "41f478c5-ed8e-4b37-8ea6-1b34a06ffa3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(7)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(7)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(10)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(9)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(9)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(7)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(10)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(10)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(7)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0774, 0.0771, 0.0765, 0.0767, 0.0768, 0.0769, 0.0765, 0.0772, 0.0768,\n",
            "        0.0768, 0.0770, 0.0774, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "[tensor(0.4973, grad_fn=<SubBackward0>), tensor(-0.5000, grad_fn=<SubBackward0>), tensor(-0.4999, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "30\n",
            "Advantage Function\n",
            "30\n",
            "return\n",
            "tensor(1.6928)\n",
            "return\n",
            "tensor(1.0530)\n",
            "return\n",
            "tensor(0.4103)\n",
            "return\n",
            "tensor(0.0212)\n",
            "return\n",
            "tensor(-0.6209)\n",
            "return\n",
            "tensor(-0.8752)\n",
            "return\n",
            "tensor(1.5991)\n",
            "return\n",
            "tensor(0.9563)\n",
            "return\n",
            "tensor(0.3143)\n",
            "return\n",
            "tensor(1.5750)\n",
            "return\n",
            "tensor(0.9322)\n",
            "return\n",
            "tensor(0.4103)\n",
            "return\n",
            "tensor(0.0319)\n",
            "return\n",
            "tensor(-0.5150)\n",
            "return\n",
            "tensor(0.1277)\n",
            "return\n",
            "tensor(0.4973)\n",
            "return\n",
            "tensor(-0.1452)\n",
            "return\n",
            "tensor(-0.7879)\n",
            "return\n",
            "tensor(-0.8657)\n",
            "return\n",
            "tensor(-1.5083)\n",
            "return\n",
            "tensor(-0.8752)\n",
            "return\n",
            "tensor(-0.8735)\n",
            "return\n",
            "tensor(-1.5162)\n",
            "return\n",
            "tensor(-0.8752)\n",
            "return\n",
            "tensor(1.6460)\n",
            "return\n",
            "tensor(1.0525)\n",
            "return\n",
            "tensor(0.4103)\n",
            "return\n",
            "tensor(-0.8786)\n",
            "return\n",
            "tensor(-1.5179)\n",
            "return\n",
            "tensor(-0.8751)\n",
            "loss\n",
            "tensor(0.0027, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0936, 0.0987, 0.0573, 0.0616, 0.0838, 0.0623, 0.0881, 0.0544, 0.0878,\n",
            "        0.0768, 0.0551, 0.0812, 0.0992], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0936, 0.0987, 0.0573, 0.0616, 0.0838, 0.0623, 0.0881, 0.0544, 0.0878,\n",
            "        0.0768, 0.0551, 0.0812, 0.0992], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0935, 0.0987, 0.0574, 0.0616, 0.0838, 0.0624, 0.0881, 0.0545, 0.0878,\n",
            "        0.0767, 0.0553, 0.0811, 0.0991], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0932, 0.0984, 0.0575, 0.0619, 0.0836, 0.0626, 0.0880, 0.0549, 0.0877,\n",
            "        0.0768, 0.0555, 0.0811, 0.0987], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0944, 0.0992, 0.0567, 0.0609, 0.0841, 0.0617, 0.0883, 0.0536, 0.0883,\n",
            "        0.0769, 0.0544, 0.0813, 0.1002], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0935, 0.0985, 0.0574, 0.0616, 0.0838, 0.0624, 0.0881, 0.0545, 0.0878,\n",
            "        0.0768, 0.0552, 0.0812, 0.0991], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0933, 0.0985, 0.0574, 0.0619, 0.0836, 0.0625, 0.0880, 0.0548, 0.0877,\n",
            "        0.0768, 0.0554, 0.0811, 0.0988], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(9)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0930, 0.0982, 0.0577, 0.0621, 0.0835, 0.0628, 0.0879, 0.0552, 0.0876,\n",
            "        0.0768, 0.0557, 0.0811, 0.0984], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0934, 0.0986, 0.0573, 0.0618, 0.0837, 0.0624, 0.0881, 0.0547, 0.0878,\n",
            "        0.0768, 0.0553, 0.0812, 0.0989], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0934, 0.0986, 0.0574, 0.0618, 0.0837, 0.0625, 0.0881, 0.0547, 0.0878,\n",
            "        0.0768, 0.0553, 0.0812, 0.0989], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0934, 0.0987, 0.0573, 0.0618, 0.0837, 0.0624, 0.0881, 0.0546, 0.0878,\n",
            "        0.0768, 0.0553, 0.0812, 0.0989], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0934, 0.0987, 0.0573, 0.0618, 0.0837, 0.0624, 0.0881, 0.0546, 0.0878,\n",
            "        0.0768, 0.0553, 0.0812, 0.0989], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(11)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0933, 0.0985, 0.0574, 0.0619, 0.0836, 0.0625, 0.0880, 0.0548, 0.0877,\n",
            "        0.0768, 0.0554, 0.0811, 0.0988], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(9)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0930, 0.0982, 0.0577, 0.0621, 0.0835, 0.0628, 0.0879, 0.0552, 0.0876,\n",
            "        0.0768, 0.0557, 0.0811, 0.0984], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(9)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0935, 0.0987, 0.0573, 0.0617, 0.0837, 0.0624, 0.0881, 0.0546, 0.0878,\n",
            "        0.0768, 0.0553, 0.0812, 0.0990], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0936, 0.0986, 0.0573, 0.0615, 0.0838, 0.0623, 0.0881, 0.0544, 0.0879,\n",
            "        0.0768, 0.0551, 0.0812, 0.0992], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0935, 0.0987, 0.0574, 0.0616, 0.0838, 0.0624, 0.0881, 0.0545, 0.0878,\n",
            "        0.0767, 0.0553, 0.0811, 0.0991], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0932, 0.0983, 0.0576, 0.0620, 0.0836, 0.0627, 0.0879, 0.0550, 0.0876,\n",
            "        0.0768, 0.0556, 0.0811, 0.0986], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(9)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0949, 0.0988, 0.0567, 0.0606, 0.0845, 0.0616, 0.0881, 0.0531, 0.0884,\n",
            "        0.0771, 0.0542, 0.0814, 0.1005], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0936, 0.0987, 0.0573, 0.0616, 0.0838, 0.0623, 0.0881, 0.0544, 0.0878,\n",
            "        0.0768, 0.0551, 0.0812, 0.0992], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0933, 0.0985, 0.0575, 0.0619, 0.0836, 0.0626, 0.0880, 0.0548, 0.0877,\n",
            "        0.0768, 0.0554, 0.0812, 0.0988], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "[tensor(0.5000, grad_fn=<SubBackward0>), tensor(0.5000, grad_fn=<SubBackward0>), tensor(0.5000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "30\n",
            "Advantage Function\n",
            "30\n",
            "return\n",
            "tensor(1.5265)\n",
            "return\n",
            "tensor(0.9510)\n",
            "return\n",
            "tensor(0.3755)\n",
            "return\n",
            "tensor(-0.7488)\n",
            "return\n",
            "tensor(-1.3243)\n",
            "return\n",
            "tensor(-0.7751)\n",
            "return\n",
            "tensor(1.5265)\n",
            "return\n",
            "tensor(0.9510)\n",
            "return\n",
            "tensor(0.3755)\n",
            "return\n",
            "tensor(-1.7216)\n",
            "return\n",
            "tensor(-1.3502)\n",
            "return\n",
            "tensor(-0.7754)\n",
            "return\n",
            "tensor(0.7482)\n",
            "return\n",
            "tensor(0.1727)\n",
            "return\n",
            "tensor(0.3755)\n",
            "return\n",
            "tensor(0.4617)\n",
            "return\n",
            "tensor(-0.1050)\n",
            "return\n",
            "tensor(-0.6804)\n",
            "return\n",
            "tensor(-0.7646)\n",
            "return\n",
            "tensor(-1.3365)\n",
            "return\n",
            "tensor(-0.7754)\n",
            "return\n",
            "tensor(1.5169)\n",
            "return\n",
            "tensor(0.9505)\n",
            "return\n",
            "tensor(0.3751)\n",
            "return\n",
            "tensor(-0.7389)\n",
            "return\n",
            "tensor(-1.3143)\n",
            "return\n",
            "tensor(-0.7492)\n",
            "return\n",
            "tensor(1.5265)\n",
            "return\n",
            "tensor(0.9510)\n",
            "return\n",
            "tensor(0.3755)\n",
            "loss\n",
            "tensor(-0.0301, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0058, 0.3474, 0.0035, 0.0037, 0.0401, 0.0023, 0.1589, 0.0017, 0.4150,\n",
            "        0.0023, 0.0042, 0.0036, 0.0115], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0059, 0.3473, 0.0036, 0.0038, 0.0407, 0.0024, 0.1588, 0.0017, 0.4139,\n",
            "        0.0024, 0.0043, 0.0036, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0057, 0.3480, 0.0034, 0.0037, 0.0401, 0.0023, 0.1579, 0.0016, 0.4161,\n",
            "        0.0023, 0.0042, 0.0035, 0.0113], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0058, 0.3471, 0.0035, 0.0037, 0.0400, 0.0023, 0.1589, 0.0017, 0.4154,\n",
            "        0.0023, 0.0042, 0.0036, 0.0114], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0059, 0.3472, 0.0036, 0.0038, 0.0405, 0.0024, 0.1587, 0.0017, 0.4145,\n",
            "        0.0024, 0.0043, 0.0036, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(6)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3483, 0.0033, 0.0035, 0.0393, 0.0022, 0.1576, 0.0016, 0.4180,\n",
            "        0.0022, 0.0040, 0.0034, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0055, 0.3483, 0.0033, 0.0035, 0.0393, 0.0022, 0.1576, 0.0016, 0.4180,\n",
            "        0.0022, 0.0040, 0.0034, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0055, 0.3492, 0.0033, 0.0035, 0.0394, 0.0022, 0.1569, 0.0015, 0.4179,\n",
            "        0.0022, 0.0040, 0.0033, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3483, 0.0033, 0.0035, 0.0393, 0.0022, 0.1576, 0.0016, 0.4180,\n",
            "        0.0022, 0.0040, 0.0034, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0057, 0.3480, 0.0034, 0.0037, 0.0401, 0.0023, 0.1579, 0.0016, 0.4161,\n",
            "        0.0023, 0.0042, 0.0035, 0.0113], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3483, 0.0033, 0.0035, 0.0393, 0.0022, 0.1576, 0.0016, 0.4180,\n",
            "        0.0022, 0.0040, 0.0034, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0055, 0.3483, 0.0033, 0.0035, 0.0393, 0.0022, 0.1576, 0.0016, 0.4180,\n",
            "        0.0022, 0.0040, 0.0034, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(12)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.0051, 0.3500, 0.0031, 0.0033, 0.0386, 0.0020, 0.1550, 0.0014, 0.4221,\n",
            "        0.0021, 0.0037, 0.0031, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.0055, 0.3483, 0.0033, 0.0035, 0.0393, 0.0022, 0.1576, 0.0016, 0.4180,\n",
            "        0.0022, 0.0040, 0.0034, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([0.0057, 0.3480, 0.0034, 0.0037, 0.0401, 0.0023, 0.1579, 0.0016, 0.4161,\n",
            "        0.0023, 0.0042, 0.0035, 0.0113], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(8)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "[tensor(0.5000, grad_fn=<SubBackward0>), tensor(0.5000, grad_fn=<SubBackward0>), tensor(0.5000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "30\n",
            "Advantage Function\n",
            "30\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2028)\n",
            "return\n",
            "tensor(-0.0013)\n",
            "return\n",
            "tensor(-1.2055)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0001)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0001)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "return\n",
            "tensor(1.2043)\n",
            "return\n",
            "tensor(0.0002)\n",
            "return\n",
            "tensor(-1.2040)\n",
            "loss\n",
            "tensor(-1.1591, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([4.8035e-12, 9.9925e-01, 2.3488e-13, 1.7782e-13, 5.3258e-06, 6.2487e-15,\n",
            "        1.5065e-08, 3.2692e-13, 7.4186e-04, 8.5303e-14, 1.6745e-11, 5.9524e-14,\n",
            "        9.3167e-12], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "for loop newdegree counter\n",
            "2\n",
            "softmax\n",
            "tensor([6.4109e-12, 9.9921e-01, 3.2386e-13, 2.3483e-13, 5.7533e-06, 8.7909e-15,\n",
            "        1.8932e-08, 4.4280e-13, 7.8166e-04, 1.1200e-13, 2.2005e-11, 8.5843e-14,\n",
            "        1.2647e-11], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "reward\n",
            "3\n",
            "[tensor(0.5000, grad_fn=<SubBackward0>), tensor(0.5000, grad_fn=<SubBackward0>), tensor(0.5000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "30\n",
            "Advantage Function\n",
            "30\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "return\n",
            "tensor(1.2042)\n",
            "return\n",
            "tensor(0.)\n",
            "return\n",
            "tensor(-1.2042)\n",
            "loss\n",
            "tensor(-4.8484e-05, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(2)\n",
        "task='node'\n",
        "#explainer=GCNPolicy(1,32,1,task=task)\n",
        "temp=20\n",
        "\n",
        "discount=1\n",
        "num_gennode_features=6805\n",
        "num_hidden_features=64\n",
        "  \n",
        "#for classes in range(2):\n",
        "  #model = GNNStack(max(3, 1), 32, 2, task=task)\n",
        "\n",
        "loss1=nn.BCELoss()\n",
        "newdegree=3\n",
        "rollout=3\n",
        "numepisodes=10\n",
        "\n",
        "for j in range(1):\n",
        "  data=deletedsubgraphdata\n",
        "  num_gennodes=len(data.y)\n",
        "  explainer=GCNPolicy()\n",
        "  opt = torch.optim.Adam(explainer.parameters(), lr=1e-2)\n",
        "  classes=classid[0]\n",
        "\n",
        "  #explainer.train()\n",
        "  \n",
        "  for epoch in range(4):\n",
        "      data=deletedsubgraphdata\n",
        "      geph=nx.Graph()\n",
        "      G=nx.Graph()\n",
        "      geph=L\n",
        "      G.add_nodes_from(geph)\n",
        "      G.add_edges_from(geph.edges)\n",
        "      #newdegree=2\n",
        "      \n",
        "      label=data.y\n",
        "      newmask=torch.ones(num_gennodes-1)\n",
        "      Actions=[]\n",
        "      Rewards=[]\n",
        "      States=[]\n",
        "      DiscountedReturns=[]\n",
        "\n",
        "      loss=0\n",
        "      \n",
        "      #print(label)\n",
        "      for episodes in range(numepisodes):\n",
        "        data=deletedsubgraphdata\n",
        "        geph=nx.Graph()\n",
        "        G=nx.Graph()\n",
        "        geph=L\n",
        "        G.add_nodes_from(geph)\n",
        "        G.add_edges_from(geph.edges)\n",
        "      #newdegree=2\n",
        "      \n",
        "        label=data.y\n",
        "        Rewards=[]\n",
        "        newmask=torch.ones(num_gennodes-1)\n",
        "\n",
        "        for k in range(newdegree):\n",
        "          #plt.figure()\n",
        "          #nx.draw_networkx(G, node_size=150,with_labels=True, node_color='pink')\n",
        "        \n",
        "          rollmask=torch.ones(num_gennodes-1)\n",
        "          print(\"for loop newdegree counter\")\n",
        "          print(k)\n",
        "          output=explainer(data)\n",
        "          States.append(data)\n",
        "          print(\"softmax\")\n",
        "        \n",
        "          print(output)\n",
        "          #o=output.numpy()\n",
        "          #o=np.multiply(o,newmask)\n",
        "          #target=output*newmask\n",
        "          #output=output*newmask\n",
        "          #print(output)\n",
        "          m = Categorical(output)\n",
        "          #print(m)\n",
        "          action = m.sample()\n",
        "          print(\"explainer output\")\n",
        "          print(action)\n",
        "          Actions.append(action)\n",
        "          a=action.item()\n",
        "          #output=output*newmask\n",
        "\n",
        "          rollmask[a]=0\n",
        "          G.add_edge(a,num_gennodes-1)\n",
        "          deg1=G.degree()\n",
        "          #print(\"degree of G while entering for loop\")\n",
        "          #print(deg1)\n",
        "          reward=0\n",
        "          #for r in range(rollout):\n",
        "            #V=nx.Graph()\n",
        "            #V.add_nodes_from(G)\n",
        "            #V.add_edges_from(G.edges)\n",
        "            #degroll=V.degree()\n",
        "            #print(\"degree of V before rollout\")\n",
        "            #print(degroll)\n",
        "            #degroll=list(degroll)\n",
        "            #degroll.sort(key=takeFirst)\n",
        "            #print(degroll)\n",
        "            #degroll=[degroll[i][1] for i in range(num_gennodes)]\n",
        "            #degcounter=degroll[num_gennodes-1]\n",
        "            #degroll=torch.FloatTensor(degroll)\n",
        "            #degroll=torch.reshape(deg,(num_gennodes,1))\n",
        "            #rolldata=pyg_utils.from_networkx(V)\n",
        "            #rolldata.x=degroll\n",
        "            #plt.figure()\n",
        "            #nx.draw_networkx(V, node_size=150,with_labels=True, node_color='white')\n",
        "          \n",
        "            #if(k!=newdegree-1):\n",
        "              #rolloutput=explainer(rolldata)\n",
        "              #rolloutput=rolloutput*rollmask\n",
        "              #rollm=Categorical(rolloutput)\n",
        "              #rollaction=rollm.sample()\n",
        "              #rollaction=rollaction.item()\n",
        "              #print(\"rollout action\")\n",
        "              #print(rollaction)\n",
        "              #V.add_edge(rollaction,num_gennodes-1)\n",
        "            #degroll=V.degree()\n",
        "            #degroll=list(degroll)\n",
        "            #degroll.sort(key=takeFirst)\n",
        "            #print(\"degree of V after rollout\")\n",
        "            #print(degroll)\n",
        "            #degroll=[degroll[i][1] for i in range(num_gennodes)]\n",
        "            #degroll=torch.FloatTensor(degroll)\n",
        "            #degroll=torch.reshape(deg,(num_gennodes,1))\n",
        "            #rolldata=pyg_utils.from_networkx(V)\n",
        "            #rolldata.x=degroll\n",
        "            #rollemb,classifierreward,rollpred=model(rolldata)\n",
        "            #pred=rollpred.argmax(dim=1)\n",
        "            #reward+=(classifierreward[num_gennodes-1][classes]-0.5)*100\n",
        "            #plt.figure()\n",
        "            #nx.draw_networkx(V, node_size=150,with_labels=True, node_color='yellow')\n",
        "            #pred.eq(label).sum().item()+\n",
        "            #pred.eq(label).sum().item()\n",
        "          #reward=reward/rollout\n",
        "         \n",
        "        \n",
        "          \n",
        "          \n",
        "         \n",
        "          expnewdata=pyg_utils.from_networkx(G)\n",
        "          expnewdata.x=data.x\n",
        "          expnewdata.y=data.y\n",
        "          data=expnewdata\n",
        "          #print(data.x)\n",
        "         \n",
        "          rollemb,classifierreward,rollpred=model(data)\n",
        "            #print(classifierreward)\n",
        "          reward=(classifierreward[num_gennodes-1][classes]-0.5)\n",
        "          Rewards.append(reward)\n",
        "          #emb1,rew1,pred1=model(data)\n",
        "          #pred=pred1.argmax(dim=1)\n",
        "          #reward2=(rew1[num_gennodes-1][classes]-0.5)\n",
        "          #reward2=0\n",
        "          #pred.eq(label).sum().item()+\n",
        "          #reward=reward+reward2\n",
        "          #opt.zero_grad()\n",
        "          #loss+=-m.log_prob(action)*(reward)\n",
        "          #if(epoch%10==0):\n",
        "          #print(\"loss\")\n",
        "          #print(loss)\n",
        "     #compute discounted returns\n",
        "      #DiscountedReturns=[]\n",
        "        for t in range(len(Rewards)):\n",
        "          print(\"reward\")\n",
        "          print(len(Rewards))\n",
        "          sum=0.0\n",
        "          for v,r in enumerate(Rewards[t:]):\n",
        "            sum+=r\n",
        "          DiscountedReturns.append(sum)\n",
        "      #print(\"DiscountedReturns\")\n",
        "      #print(DiscountedReturns)\n",
        "      print(Rewards)\n",
        "      #for x in range(1):\n",
        "        \n",
        "        #print(\"enter loss calculation\")\n",
        "        #States=torch.FloatTensor(States)\n",
        "        #probs=explainer(States)\n",
        "        #dist=torch.distributions.Categorical(probs=probs)\n",
        "        #log_prob = dist.log_prob(Actions)\n",
        "        #loss=-(log_prob*DiscountedReturns).sum()/num_episodes\n",
        "        #opt.zero_grad()\n",
        "        #opt.step()\n",
        "\n",
        "\n",
        "      loss=0\n",
        "      print(\"DiscountedReturns\")\n",
        "      print(len(DiscountedReturns))\n",
        "      DiscountedReturns=torch.tensor(DiscountedReturns)\n",
        "      DiscountedReturns=(DiscountedReturns-DiscountedReturns.mean())/(DiscountedReturns.std()+1e-9)\n",
        "      #compute baseline for advantage functions\n",
        "      b1=0\n",
        "      b2=0\n",
        "      ele=0\n",
        "      ele1=0\n",
        "      Advantage=[]\n",
        "      countstep=0\n",
        "      while(ele<len(DiscountedReturns)):\n",
        "        if(ele%2==0):\n",
        "          b1+=DiscountedReturns[ele]\n",
        "          countstep+=1\n",
        "        else:\n",
        "          b2+=DiscountedReturns[ele]\n",
        "        ele+=1\n",
        "      b1=b1/countstep\n",
        "      b2=b2/countstep\n",
        "      while(ele1<len(DiscountedReturns)):\n",
        "        if(ele1%2==0):\n",
        "         Advantage.append(DiscountedReturns[ele1]-b1)\n",
        "        else:\n",
        "          Advantage.append(DiscountedReturns[ele1]-b2)\n",
        "        ele1+=1\n",
        "      print(\"Advantage Function\")\n",
        "      print(len(Advantage))\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      for State, Action, R in zip(States, Actions, DiscountedReturns):\n",
        "        print(\"return\")\n",
        "        print(R)\n",
        "        probs = explainer(State)\n",
        "        dist = torch.distributions.Categorical(probs=probs)    \n",
        "        log_prob = dist.log_prob(Action)\n",
        "        \n",
        "        loss+= - log_prob*R\n",
        "        \n",
        "      newloss=loss/numepisodes\n",
        "      print(\"loss\")\n",
        "      print(newloss)\n",
        "      opt.zero_grad()\n",
        "      newloss.backward()\n",
        "      opt.step()\n",
        "\n",
        "  \n",
        "  #nx.draw_networkx(G, node_size=150,with_labels=False, node_color=color_map)\n",
        "  plt.figure(j)\n",
        "  nx.draw_networkx(G, node_size=150,node_color=label,with_labels=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following segment prints the class score of the query node after its neighborhood has been generated"
      ],
      "metadata": {
        "id": "H8J_Lhmkc5eo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmRVpef66Xu1",
        "outputId": "28ea27b8-2510-45f7-f8ce-0b4c52d6d42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.3004e-13, 2.6846e-27, 2.0419e-19, 1.1518e-33, 1.3116e-16, 3.2593e-11,\n",
            "        1.3809e-17, 3.3801e-25, 3.4653e-32, 8.9343e-29, 1.1562e-22, 1.0000e+00,\n",
            "        4.2622e-24, 3.1853e-27, 3.6755e-16], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "emb,reward,pred=model(data)\n",
        "print(reward[-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Node Classifier Explainer CoAuthorCS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}