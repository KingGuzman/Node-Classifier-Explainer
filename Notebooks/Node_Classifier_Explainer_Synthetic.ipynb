{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingGuzman/Node-Classifier-Explainer/blob/main/Node_Classifier_Explainer_Synthetic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run all the cells one after another. "
      ],
      "metadata": {
        "id": "YiIitz4NyN7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o-N2dk6oGMn",
        "outputId": "ccb2c820-ce0e-418c-b21c-36b5f5cedce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 25.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MlFlxfL5dgn2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import torch_geometric as torch_geometric\n",
        "import math\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import GCNConv,GINConv\n",
        "from torch.distributions import Bernoulli,Categorical\n",
        "import matplotlib.cm as cmx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FynD3_oz2Mtf",
        "outputId": "ba4cc616-29b9-45c1-fc6a-6424bd0445af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "from collections import Iterable\n",
        "def flatten(lis):\n",
        "     for item in lis:\n",
        "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
        "             for x in flatten(item):\n",
        "                 yield x\n",
        "         else:        \n",
        "             yield item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSGfDfMzjTj3"
      },
      "source": [
        "Code below generates Erdos-Renyi graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "lLgPSGazSEh1",
        "outputId": "a4728980-0cbe-4aec-f8fe-551b42f386e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "8340\n",
            "15467\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#G=nx.empty_graph(10)\n",
        "dataset=[]\n",
        "graphlist=[]\n",
        "\n",
        "num_node_features=1\n",
        "count1=0\n",
        "totalnode=0\n",
        "random.seed(2)\n",
        "for numdata in range(1000):\n",
        "  num_nodes=random.randint(1,30)\n",
        "  totalnode+=num_nodes\n",
        "  \n",
        "  y=np.ones(num_nodes)\n",
        "\n",
        "  #Cycle.add_nodes_from([i in range(0,100)])\n",
        "  p=math.ceil(random.uniform(1,2*num_nodes))\n",
        "  Cycle = nx.gnm_random_graph(num_nodes,p,seed=2)\n",
        "  #numcycles=random.randint(1,6)\n",
        "  #startind=random.randint(1,5)\n",
        "  L=nx.cycle_basis(Cycle)\n",
        "  #print(L)\n",
        "  lis=list(set(list(flatten(L))))\n",
        "  #print(lis)\n",
        "  for l in lis:\n",
        "\n",
        "    y[l]=0\n",
        " \n",
        "  \n",
        "  \n",
        "\n",
        " \n",
        "  data=pyg_utils.from_networkx(Cycle)\n",
        "  count1+=np.count_nonzero(y)\n",
        "\n",
        "  #print(y)\n",
        "  y=torch.from_numpy(y)\n",
        "  y=y.long()\n",
        "  data.y=y\n",
        "  deg=Cycle.degree()\n",
        "  deg=list(deg)\n",
        "  deg=[deg[i][1] for i in range(num_nodes)]\n",
        "  deg=torch.FloatTensor(deg)\n",
        "  deg=torch.reshape(deg,(num_nodes,1))\n",
        "\n",
        "  #print(deg.shape)\n",
        "  \n",
        "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
        "  data.x=deg\n",
        "  dataset.append(data)\n",
        "\n",
        "  #print(data.edge_index)\n",
        "\n",
        "\n",
        "  \n",
        "# illustrate graph\n",
        "nx.draw_networkx(Cycle, node_size=150, node_color='yellow')\n",
        "print(y)\n",
        "\n",
        "print(count1)\n",
        "print(totalnode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPwgFb_KjGKO"
      },
      "source": [
        "Code below generates trees\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "Z0VJAMCHCrZG",
        "outputId": "43f6d237-74d4-456e-9a05-42280d3b2fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "13298\n",
            "13298\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#G=nx.empty_graph(10)\n",
        "#dataset=[]\n",
        "#graphlist=[]\n",
        "\n",
        "num_node_features=1\n",
        "count1=0\n",
        "totalnode=0\n",
        "for numdata in range(500):\n",
        "  num_nodes=random.randint(1,50)\n",
        "  totalnode+=num_nodes\n",
        "  \n",
        "  y=np.ones(num_nodes)\n",
        "\n",
        "  #Cycle.add_nodes_from([i in range(0,100)])\n",
        "  p=math.ceil(random.uniform(1,5))\n",
        "  Cycle = nx.full_rary_tree(p,num_nodes)\n",
        "  #numcycles=random.randint(1,6)\n",
        "  #startind=random.randint(1,5)\n",
        "  L=nx.cycle_basis(Cycle)\n",
        "  #print(L)\n",
        "  lis=list(set(list(flatten(L))))\n",
        "  #print(lis)\n",
        "  for l in lis:\n",
        "\n",
        "    y[l]=0\n",
        " \n",
        "  \n",
        "  \n",
        "\n",
        " \n",
        "  data=pyg_utils.from_networkx(Cycle)\n",
        "  count1+=np.count_nonzero(y)\n",
        "\n",
        "  #print(y)\n",
        "  y=torch.from_numpy(y)\n",
        "  y=y.long()\n",
        "  data.y=y\n",
        "  deg=Cycle.degree()\n",
        "  deg=list(deg)\n",
        "  deg=[deg[i][1] for i in range(num_nodes)]\n",
        "  deg=torch.FloatTensor(deg)\n",
        "  deg=torch.reshape(deg,(num_nodes,1))\n",
        "\n",
        "  #print(deg.shape)\n",
        "  \n",
        "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
        "  data.x=deg\n",
        "  dataset.append(data)\n",
        "\n",
        "  #print(data.edge_index)\n",
        "\n",
        "\n",
        "  \n",
        "# illustrate graph\n",
        "nx.draw_networkx(Cycle, node_size=150, node_color='yellow')\n",
        "print(y)\n",
        "\n",
        "print(count1)\n",
        "print(totalnode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code below generates wheel graphs"
      ],
      "metadata": {
        "id": "vUj0juYW7BMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "D71fZzfTCBNA",
        "outputId": "3eb59bc2-700b-42c7-844b-d18597cc91d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "0\n",
            "5851\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "num_node_features=1\n",
        "count1=0\n",
        "totalnode=0\n",
        "for numdata in range(500):\n",
        "  num_nodes=random.randint(3,20)\n",
        "  totalnode+=num_nodes\n",
        "  \n",
        "  y=np.ones(num_nodes)\n",
        "\n",
        "  #Cycle.add_nodes_from([i in range(0,100)])\n",
        "  p=math.ceil(random.uniform(1,5))\n",
        "  Cycle = nx.wheel_graph(num_nodes)\n",
        "  #numcycles=random.randint(1,6)\n",
        "  #startind=random.randint(1,5)\n",
        "  L=nx.cycle_basis(Cycle)\n",
        "  #print(L)\n",
        "  lis=list(set(list(flatten(L))))\n",
        "  \n",
        "  #print(lis)\n",
        "  for l in lis:\n",
        "\n",
        "    y[l]=0\n",
        " \n",
        "  \n",
        "  \n",
        "\n",
        " \n",
        "  data=pyg_utils.from_networkx(Cycle)\n",
        "  count1+=np.count_nonzero(y)\n",
        "\n",
        "  #print(y)\n",
        "  y=torch.from_numpy(y)\n",
        "  y=y.long()\n",
        "  data.y=y\n",
        "  deg=Cycle.degree()\n",
        "  deg=list(deg)\n",
        "  deg=[deg[i][1] for i in range(num_nodes)]\n",
        "  deg=torch.FloatTensor(deg)\n",
        "  deg=torch.reshape(deg,(num_nodes,1))\n",
        "\n",
        "  #print(deg.shape)\n",
        "  \n",
        "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
        "  data.x=deg\n",
        "  dataset.append(data)\n",
        "\n",
        "  #print(data.edge_index)\n",
        "\n",
        "\n",
        "  \n",
        "# illustrate graph\n",
        "nx.draw_networkx(Cycle, node_size=150, node_color='yellow')\n",
        "print(y)\n",
        "\n",
        "print(count1)\n",
        "print(totalnode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDD2IDJyehme",
        "outputId": "ec3ebf79-3589-454b-961b-a150290a8d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))\n",
        "random.shuffle(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_yhx987YMQ32"
      },
      "outputs": [],
      "source": [
        "class GNNStack(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, task='node'):\n",
        "        super(GNNStack, self).__init__()\n",
        "        self.task = task\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
        "        self.lns = nn.ModuleList()\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        for l in range(3):\n",
        "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
        "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "        if not (self.task == 'node' or self.task == 'graph'):\n",
        "            raise RuntimeError('Unknown task.')\n",
        "\n",
        "        self.dropout = 0.25\n",
        "        self.num_layers = 3\n",
        "\n",
        "    def build_conv_model(self, input_dim, hidden_dim):\n",
        "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
        "        if self.task == 'node':\n",
        "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
        "        else:\n",
        "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        if data.num_node_features == 0:\n",
        "          x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            emb = x\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            if not i == self.num_layers - 1:\n",
        "                x = self.lns[i](x)\n",
        "\n",
        "        if self.task == 'graph':\n",
        "            x = pyg_nn.global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "        reward=F.softmax(x,dim=1)\n",
        "        \n",
        "\n",
        "        return emb,reward,F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)\n",
        "        #F.nll_loss(pred, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nccabIHQ36gc"
      },
      "outputs": [],
      "source": [
        "def train(dataset, task):\n",
        "    torch.manual_seed(3)\n",
        "    if task == 'graph':\n",
        "        data_size = len(dataset)\n",
        "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
        "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
        "    else:\n",
        "         loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(max(num_node_features, 1), 64, 2, task=task)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model=model.to(device)\n",
        "    \n",
        "    # train\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            #print(batch.train_mask, '----')\n",
        "            opt.zero_grad()\n",
        "            batch=batch.to(device)\n",
        "            embedding,reward,pred = model(batch)\n",
        "            \n",
        "            label = batch.y\n",
        "            #label=label.float()\n",
        "            #labelonehot=torch.nn.functional.one_hot(label,num_classes=2)\n",
        "            #labelonehot=labelonehot.float()\n",
        "            #labelonehot=torch.zeros_like(label)\n",
        "            #labelonehot=labelonehot.scatter(1,label, 1)\n",
        "            \n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        #writer.add_scalar(\"loss\", total_loss, epoch)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "          print(epoch)\n",
        "          #print(reward)\n",
        "          print(loss*100)\n",
        "          #print(pred.shape)\n",
        "            #test_acc = test(test_loader, model)\n",
        "            #print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
        "                #epoch, total_loss, test_acc))\n",
        "            #writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QL4U3tsU4vkc"
      },
      "outputs": [],
      "source": [
        "def test(loader, model, task='node'):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total=0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            emb, reward,pred = model(data)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            label = data.y\n",
        "            \n",
        "\n",
        "        #if task == 'node':\n",
        "            #mask = data.val_mask if is_validation else data.test_mask\n",
        "            # node classification: only evaluate on nodes in test set\n",
        "            #pred = pred[mask]\n",
        "            #label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "        total+=len(label)\n",
        "    \n",
        "    #if task == 'graph':\n",
        "     #   total = len(loader.dataset) \n",
        "    #else:\n",
        "        #total = len(loader.dataset)*50\n",
        "        \n",
        "    return correct / total,total,correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ2yXTaYyM1a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Classifier"
      ],
      "metadata": {
        "id": "iyvtdmF87eqy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NiFQBnEvidU",
        "outputId": "82f07f8b-b788-46da-e551-74877e32e18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor(20.7206, grad_fn=<MulBackward0>)\n",
            "20\n",
            "tensor(22.3719, grad_fn=<MulBackward0>)\n",
            "40\n",
            "tensor(27.6279, grad_fn=<MulBackward0>)\n",
            "60\n",
            "tensor(9.0178, grad_fn=<MulBackward0>)\n",
            "80\n",
            "tensor(11.4022, grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "task = 'node'\n",
        "\n",
        "model = train(dataset, task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XRXDnv8nReN",
        "outputId": "27a1e9fa-fd57-46b0-c22f-c544d745ec8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9871411483253588\n",
            "3344\n",
            "3301\n"
          ]
        }
      ],
      "source": [
        "testloader=DataLoader(dataset[1801:], batch_size=1, shuffle=True)\n",
        "accuracy,total,correct=test(testloader,model)\n",
        "#embedding,reward,pred = model(data)\n",
        "#pred=pred.argmax(dim=1)\n",
        "#label=data.y\n",
        "#correct=pred.eq(label).sum().item()\n",
        "#total=len(label)\n",
        "#accuracy=correct/total\n",
        "print(accuracy)\n",
        "print(total)\n",
        "print(correct)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Neighborhood Generator Policy Network"
      ],
      "metadata": {
        "id": "yrsR8iHZ7jhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IFcyiiD_2VcI"
      },
      "outputs": [],
      "source": [
        "class GCNPolicy(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GCNPolicy, self).__init__()\n",
        "    self.conv1=GCNConv(num_gennode_features,num_hidden_features)\n",
        "    self.conv2=GCNConv(num_hidden_features,num_hidden_features)\n",
        "    self.conv3=GCNConv(num_hidden_features,num_gennode_features)\n",
        "    self.lin1=nn.Sequential(nn.Linear((num_gennodes)*num_gennode_features, num_hidden_features),nn.ReLU(),nn.Linear(num_hidden_features,num_hidden_features),nn.ReLU(),nn.Linear(num_hidden_features,num_gennodes-1))\n",
        "  def forward(self,gendata):\n",
        "        x, edge_index = gendata.x, gendata.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x=F.relu(x)\n",
        "        x=self.conv3(x,edge_index)\n",
        "        x=F.relu(x)\n",
        "        x=torch.flatten(x)\n",
        "        x=self.lin1(x)\n",
        "        x=F.softmax(x/temp)\n",
        "        #x=Bernoulli(x)\n",
        "        #action=x.sample()\n",
        "       \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNJoR7RtcbfP"
      },
      "source": [
        "Making templates and query nodes. Query nodes are in green and template nodes are red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "g8jhjQ3tazFg",
        "outputId": "a7ea5c99-d79c-4246-ab17-b3996b5c0a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random p generated is 2\n",
            "random p generated is 2\n",
            "random p generated is 4\n",
            "random p generated is 5\n",
            "22\n",
            "22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAACxCAYAAACCwvy/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAKk0lEQVR4nO3de2yT9R7H8c/Tdm3XTjLkZgjTEpZluC0hUCKXJVyUeEEZxkSRxGQgRzMMioYc4oFAJuHEHJWTYJiBBPFCGAluGf+YKAS84IlmlyNZ5iIoCxd1yLjEtStbn/Z7/njOgG59Slu6PuuPzyshQJ/t4bvw5sevl/XRRAREKrFZPQBRpjFqUg6jJuUwalIOoyblMGpSjiPRwfHjx4vP58vSKESpaW1t7RGRCUNvTxi1z+dDS0vLyE1FdAc0TTsb73ZuP0g5KUf9Y/ePWH5wOcb9axyK/l2Erce3ore/dyRmI0qLluhpcr/fL7duP451HcNT9U8hFA5BYHye2+GGr9CH5r81o8BZMOIDEw3SNK1VRPxDb096pRYRVDdVoy/cdyNoALiuX8fZa2dR11yXoVGJ7kzSUXdc6sCV0JW4x0J6CPv+uy9jQxHdiaSjvq5fh12zmx4P6aGMDER0p5KOumJiBaKIxj3msDnwePHjGRuK6E4kHbXL4cKWBVvgyfMMO5bvyMfGyo0ZHYwoXQmffBlqw9wNcNqdqP2qFuFIGOFoGGUTy/Dhsg/hK/SN0IhEqUnpIb1B4UgYXde6UOAswOR7Jo/kfESmzB7SS2mlHpRnz0PJuJI7n4poBPBpclIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalIOoyblMGpSDqMm5TBqUg6jJuUwalKOw+oBKE39/cBnnwGNjYDDAaxcCTz5JGC3Wz2Z5Rh1Lrp6FZg3D7hwAQgEjNs+/xyYMQM4cgRwu62dLx0iwA8/GF9TaSlQXp72qbj9yEWvvgqcOXMzaMD4dWsr8Pbb1s2Vrp9+AoqLgSVLgBdfBB56CJgzB7h4Ma3TaSJietDv90tLS0u6o9JI6O8HCguB69fjH584Me0YLNHbC/h8xv8+t7bocADTpwMnTwKaFvdTNU1rFRH/0Nu5Uuea3t7Yv/yhrl7N3iyZsH+/8Q916Nek60BXF3DiRMqnZNS5ZuxYID/f9HD/lClZHCYDvvsOCAbjHwuHjS1Vihh1rrHbgQ0bAI9n2KGw04l1PT1Yv349rl27ZsFwqestKEDEZpJhXh4wfnzK52TUuejNN4EXXjAe5fB6gYICwO1G3qZN2P7rr+jr60NpaSn27NmDSCRi9bRxdXR0YPXq1VhSX28edTQKLF+e+slFxPTHrFmzhEax8+dF9u0T+fRTkUuXYg61tbVJZWWlzJgxQ7755htr5hsiGo3K8ePH5YknnpBJkybJtm3bpKenR2T7dhGPR8RmEwFEHA7j942NCc8HoEXidMuoFRaNRqW+vl6Kiorkueeek3PnzlkyRzgcloMHD4rf75eSkhLZs2ePhEKh2A/6/nuRlStF5s4VWbtW5Oefb3teRn0XCwQCsmXLFrn33nultrZW+vr6svbn7ty5U6ZOnSqVlZXS1NQkkUgkY+c3i5p76ruA1+tFbW0tWltb0d7ejunTp+PQoUPGqjYCLl68iM2bN8Pn8+Hrr7/GgQMH8O2336Kqqgo2s/1zJsUrXbhSK+3YsWNSUVEhCxculJMnT2bsvJ2dnbJmzRopLCyUmpoaOX36dMbOHQ+4UtOgRYsWoa2tDc8++yyWLFmCtWvXoqenJ61ziQhOnDiBqqoqLFiwAFOmTMGpU6dQV1eH4uLiDE+eHEZ9l3I4HKipqUFnZyfsdjsefPBBvP/++9B13fiA7m5g82Zg5kygshL4+GNgYODG50ciETQ0NGDu3Lmorq7GY489hq6uLmzduhUTJkyw6Kv6v3jLt3D7cddpb2+XxYsXS1lZmfxn716RwkIRl8t4LAEQ8XpF5s2T4JUrsmvXLpk2bZrMmTNHGhoaRNd1S2aGyfaDLz0lAEB5eTmOHj2KpqYmeFesQHRgIPa/8WAQA83N+GdREdoffhgfffQR5s+fD83kxUZWYtR0g6ZpeHr2bIjNhnipOsNhbJk8Gc7Dh7M+Wyq4p6ZYV69Cy8szPewMhbI4THoYNcUqLjZecxGPphkv3h/lGDXFys8H1q+P+ypA5Ocbj4iMcoyahnvrLeDllwG3G702GyJeLzBuHFBfD8yebfV0t8WoaTibDdixA/j9d7w+eTLOf/CB8bj1smVWT5YURk3mxo7FFwBsCxYY3zOYIxg1JRQMBuH1eq0eIyWMmhJi1KQUXdeh6zpcLpfVo6SEUZOpwVV6ND4VngijJlO5uPUAGDUlwKhJOcFgEAUFBVaPkTJGTaa4UpNyAoEAoya1cKUm5TBqUg6jJuUwalLOwJUrGJOD14/JndcTUvY0NgIbN+LvZ84Yv29vB3buBKZOtXauJHGlpliffGK89/Uvv8AejcIejRpX/po9G/jjD6unSwqjppt0HXjjDaCvL/b2aBT46y/gvfesmStFjJpu6ugwLioUTzhsXIw0BzBquul2b7ObjbfhzYDcmJKyo6ws/lsjAIDLBTz/fHbnSROjpptsNqCubnjYDodxqbvXX7dmrhQxaor1zDPA4cPArFmIAtDz8oCVK4G2trQu/2YFPk5Nwz3yCNDSgvXr1mFacTFee+01qydKCVdqMuVyuzFwyxut5wpGTaacTif6zR7iG8UYNZlyOp1cqUktLpeLUZNauP0g5XClJuVwpSbl8I4iKYfbD1IOtx+kHK7UpByu1KQc3lEktfz5Jx5obETN6dPGN+PmwJVuBzFqGu7gQcDnw/27d+P57m7glVeAoiLjrRJyAKOmWF1dwOrVQCgE2+B+OhAALl8GHn0UiESsnS8JjJpi7d5tHm4gABw9mt150sCoKVZnJ2B251DXjZV8lGPUFKuiAnA64x9zOIDi4uzOkwZGTbFeein+JZs1DSgsBBYvzv5MKWLUFOv++4EDBwCPB+LxQADgnnuAiROBL7/MiTe0Gf0TUvZVVQEXLiD6zjvYpmnA3r3AuXNAaanVkyVFExHTg36/X1paWrI4Do0mIgK73Q5d12EbhSu0pmmtIuIfevvom5RGDU3TcvKpckZNCeXii5oYNSXElZqUk4uvqWbUlBC3H6QcrtSkHK7UpBzeUSTlcPtB6giHgYYG/OPMGRTt2AE0N1s9UdIYNQ3X0wOUlwPV1Vja3Y0pTU3AwoXAqlVAgpdVjBaMmoZbtcr4ZoBAAABgi0aNC4YeOgTs32/xcLfHqCnW5cvAkSPG9mOoYBB4993sz5QiRk2xurvNv/MFAH77LXuzpIlRU6yiovir9KCSkuzNkiZGTbHGjAFWrADc7uHHPB5g06bsz5QiRk3D7doFVFYaETudxs9utxH00qVWT3dbvDgoDefxGHcW29qAr74C8vOBp58G7rvP6smSwqjJ3MyZxo8cw+0HKYdRk3IYNSmHUZNyGDUph1GTchK+Q5OmaZcAnM3eOEQpeUBEJgy9MWHURLmI2w9SDqMm5TBqUg6jJuUwalLO/wCGTPHCuxBKtwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "num_gennode_features=1\n",
        "num_hidden_features=64\n",
        "temp=0.1\n",
        "dataset=[]\n",
        "graphlist=[]\n",
        "\n",
        "num_node_features=1\n",
        "count1=0\n",
        "totalnode=0\n",
        "num_gennodes=4\n",
        "po=3\n",
        "graphlen=[]\n",
        "\n",
        "for numdata in range(4):\n",
        " \n",
        "  num_nodes=num_gennodes\n",
        "  graphlen.append(num_nodes)\n",
        "  k=num_gennodes-1\n",
        "  totalnode+=num_nodes\n",
        "  \n",
        "  y=np.ones(num_nodes)\n",
        "\n",
        "  #Cycle.add_nodes_from([i in range(0,100)])\n",
        "\n",
        "  random.seed(po)\n",
        "  p=math.ceil(random.uniform(1,5))\n",
        "  \n",
        "  print(\"random p generated is\",p)\n",
        "  Cycle = nx.gnm_random_graph(num_nodes-1,p,seed=3)\n",
        "  Cycle.add_node(k)\n",
        "  #numcycles=random.randint(1,6)\n",
        "  #startind=random.randint(1,5)\n",
        "  L=nx.cycle_basis(Cycle)\n",
        "  #print(L)\n",
        "  lis=list(set(list(flatten(L))))\n",
        "  #print(lis)\n",
        "  for l in lis:\n",
        "\n",
        "    y[l]=0\n",
        " \n",
        "  \n",
        "  \n",
        "\n",
        " \n",
        "  data=pyg_utils.from_networkx(Cycle)\n",
        "  adj=data.edge_index\n",
        "  #print(\"adjacency matrix\")\n",
        "  #print(adj)\n",
        "  count1+=np.count_nonzero(y)\n",
        "  y[num_nodes-1]=0\n",
        "  #print(y)\n",
        "  y=torch.from_numpy(y)\n",
        "  y=y.long()\n",
        "  data.y=y\n",
        "  deg=Cycle.degree()\n",
        "  deg=list(deg)\n",
        "  deg=[deg[i][1] for i in range(num_nodes)]\n",
        "  deg=torch.FloatTensor(deg)\n",
        "  deg=torch.reshape(deg,(num_nodes,1))\n",
        "\n",
        "  #print(deg.shape)\n",
        "  \n",
        "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
        "  data.x=deg\n",
        "  dataset.append(data)\n",
        "  graphlist.append(Cycle)\n",
        "\n",
        "  #print(data.edge_index)\n",
        "\n",
        "\n",
        "  \n",
        "# illustrate graph\n",
        "  color_map = []\n",
        "  for node in Cycle:\n",
        "      if node < num_gennodes-1:\n",
        "          color_map.append('red')\n",
        "      else: \n",
        "          color_map.append('green')      \n",
        "#nx.draw(G, node_color=color_map, with_labels=True)\n",
        "  plt.figure(numdata,figsize=(3,3))\n",
        "  nx.draw_networkx(Cycle, node_size=50, node_color=color_map, with_labels=False)\n",
        "  num_gennodes=num_gennodes+1\n",
        "  po=po+1\n",
        "print(count1)\n",
        "print(totalnode)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1w4BELXNRIJ",
        "outputId": "f89c93a3-eb29-42b6-e16b-cc7c71b95653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.3481e-13, 1.0000e+00],\n",
            "        [3.3481e-13, 1.0000e+00],\n",
            "        [5.4822e-10, 1.0000e+00],\n",
            "        [1.4553e-10, 1.0000e+00],\n",
            "        [9.7025e-10, 1.0000e+00],\n",
            "        [4.9613e-15, 1.0000e+00],\n",
            "        [4.0253e-21, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 0])\n"
          ]
        }
      ],
      "source": [
        "emb,rew,pred=model(data)\n",
        "print(rew)\n",
        "print(pred.argmax(dim=1))\n",
        "print(rew.argmax(dim=1))\n",
        "print(data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoXvhVFoch3P"
      },
      "source": [
        "Below cell contains code for explainer training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5cPqSAlC7Vvx"
      },
      "outputs": [],
      "source": [
        "def takeFirst(elem):\n",
        "  return elem[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgcgrCPoRBX4"
      },
      "source": [
        "Generating Optimal Neighborhoods using the Policy Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VGlh91dqRLAm",
        "outputId": "c9fa15f8-3ba3-4a7a-8fe2-cdb9f10f5530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.3544, 0.3410, 0.3046], grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
            "for loop newdegree counter\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1355, 0.2864, 0.1797, 0.1461, 0.0766, 0.1757],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.2696, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(1.0771)\n",
            "return\n",
            "tensor(1.0771)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(1.0771)\n",
            "return\n",
            "tensor(1.0771)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(-0.1892)\n",
            "return\n",
            "tensor(-0.1892)\n",
            "return\n",
            "tensor(1.0886)\n",
            "return\n",
            "tensor(1.0886)\n",
            "return\n",
            "tensor(0.7460)\n",
            "return\n",
            "tensor(0.7460)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(-1.1145)\n",
            "return\n",
            "tensor(0.6584)\n",
            "return\n",
            "tensor(0.6584)\n",
            "loss\n",
            "tensor(0.2275, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1351, 0.2771, 0.1839, 0.1444, 0.0806, 0.1789],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.1207, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-0.2804)\n",
            "return\n",
            "tensor(-0.2804)\n",
            "return\n",
            "tensor(-1.2559)\n",
            "return\n",
            "tensor(-1.2559)\n",
            "return\n",
            "tensor(0.7979)\n",
            "return\n",
            "tensor(0.7979)\n",
            "return\n",
            "tensor(1.6114)\n",
            "return\n",
            "tensor(1.6114)\n",
            "return\n",
            "tensor(-1.2559)\n",
            "return\n",
            "tensor(-1.2559)\n",
            "return\n",
            "tensor(-1.2559)\n",
            "return\n",
            "tensor(-1.2559)\n",
            "return\n",
            "tensor(-0.2804)\n",
            "return\n",
            "tensor(-0.2804)\n",
            "return\n",
            "tensor(0.7979)\n",
            "return\n",
            "tensor(0.7979)\n",
            "return\n",
            "tensor(0.3234)\n",
            "return\n",
            "tensor(0.3234)\n",
            "return\n",
            "tensor(0.7979)\n",
            "return\n",
            "tensor(0.7979)\n",
            "loss\n",
            "tensor(-0.5712, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1330, 0.2762, 0.1884, 0.1401, 0.0800, 0.1822],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.0318, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.0890)\n",
            "return\n",
            "tensor(0.0890)\n",
            "return\n",
            "tensor(0.2956)\n",
            "return\n",
            "tensor(0.2956)\n",
            "return\n",
            "tensor(0.6499)\n",
            "return\n",
            "tensor(0.6499)\n",
            "return\n",
            "tensor(0.6499)\n",
            "return\n",
            "tensor(0.6499)\n",
            "return\n",
            "tensor(-0.1740)\n",
            "return\n",
            "tensor(-0.1740)\n",
            "return\n",
            "tensor(0.3119)\n",
            "return\n",
            "tensor(0.3119)\n",
            "return\n",
            "tensor(-2.8128)\n",
            "return\n",
            "tensor(-2.8128)\n",
            "return\n",
            "tensor(0.3119)\n",
            "return\n",
            "tensor(0.3119)\n",
            "return\n",
            "tensor(0.0285)\n",
            "return\n",
            "tensor(0.0285)\n",
            "return\n",
            "tensor(0.6499)\n",
            "return\n",
            "tensor(0.6499)\n",
            "loss\n",
            "tensor(0.2384, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1319, 0.2709, 0.1931, 0.1369, 0.0807, 0.1866],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.9000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.7634)\n",
            "return\n",
            "tensor(0.7634)\n",
            "return\n",
            "tensor(0.7634)\n",
            "return\n",
            "tensor(0.7634)\n",
            "return\n",
            "tensor(0.4918)\n",
            "return\n",
            "tensor(0.4918)\n",
            "return\n",
            "tensor(0.1838)\n",
            "return\n",
            "tensor(0.1838)\n",
            "return\n",
            "tensor(0.2871)\n",
            "return\n",
            "tensor(0.2871)\n",
            "return\n",
            "tensor(0.0366)\n",
            "return\n",
            "tensor(0.0366)\n",
            "return\n",
            "tensor(0.4918)\n",
            "return\n",
            "tensor(0.4918)\n",
            "return\n",
            "tensor(0.7634)\n",
            "return\n",
            "tensor(0.7634)\n",
            "return\n",
            "tensor(-1.8908)\n",
            "return\n",
            "tensor(-1.8908)\n",
            "return\n",
            "tensor(-1.8908)\n",
            "return\n",
            "tensor(-1.8908)\n",
            "loss\n",
            "tensor(-0.0009, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1312, 0.2655, 0.1979, 0.1330, 0.0812, 0.1912],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.1207, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.8739)\n",
            "return\n",
            "tensor(0.8739)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(0.4335)\n",
            "return\n",
            "tensor(0.4335)\n",
            "return\n",
            "tensor(0.8739)\n",
            "return\n",
            "tensor(0.8739)\n",
            "return\n",
            "tensor(1.0420)\n",
            "return\n",
            "tensor(1.0420)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(0.6820)\n",
            "return\n",
            "tensor(0.6820)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(-1.1800)\n",
            "return\n",
            "tensor(0.8146)\n",
            "return\n",
            "tensor(0.8146)\n",
            "loss\n",
            "tensor(0.0410, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1308, 0.2605, 0.2001, 0.1296, 0.0818, 0.1971],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.2384, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(0.9164)\n",
            "return\n",
            "tensor(0.9164)\n",
            "return\n",
            "tensor(0.8559)\n",
            "return\n",
            "tensor(0.8559)\n",
            "return\n",
            "tensor(1.0880)\n",
            "return\n",
            "tensor(1.0880)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(0.5926)\n",
            "return\n",
            "tensor(0.5926)\n",
            "return\n",
            "tensor(0.7205)\n",
            "return\n",
            "tensor(0.7205)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(-1.1804)\n",
            "return\n",
            "tensor(0.5482)\n",
            "return\n",
            "tensor(0.5482)\n",
            "loss\n",
            "tensor(0.2488, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1318, 0.2534, 0.2000, 0.1279, 0.0834, 0.2036],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.9000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-8.4226e-05)\n",
            "return\n",
            "tensor(-8.4226e-05)\n",
            "return\n",
            "tensor(0.1398)\n",
            "return\n",
            "tensor(0.1398)\n",
            "return\n",
            "tensor(-0.0705)\n",
            "return\n",
            "tensor(-0.0705)\n",
            "return\n",
            "tensor(-8.4226e-05)\n",
            "return\n",
            "tensor(-8.4226e-05)\n",
            "return\n",
            "tensor(0.4171)\n",
            "return\n",
            "tensor(0.4171)\n",
            "return\n",
            "tensor(0.5130)\n",
            "return\n",
            "tensor(0.5130)\n",
            "return\n",
            "tensor(0.7849)\n",
            "return\n",
            "tensor(0.7849)\n",
            "return\n",
            "tensor(0.5130)\n",
            "return\n",
            "tensor(0.5130)\n",
            "return\n",
            "tensor(0.5130)\n",
            "return\n",
            "tensor(0.5130)\n",
            "return\n",
            "tensor(-2.8100)\n",
            "return\n",
            "tensor(-2.8100)\n",
            "loss\n",
            "tensor(0.0364, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1329, 0.2481, 0.1971, 0.1269, 0.0846, 0.2105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.5710, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.6925)\n",
            "return\n",
            "tensor(0.6925)\n",
            "return\n",
            "tensor(0.1555)\n",
            "return\n",
            "tensor(0.1555)\n",
            "return\n",
            "tensor(0.9428)\n",
            "return\n",
            "tensor(0.9428)\n",
            "return\n",
            "tensor(0.0367)\n",
            "return\n",
            "tensor(0.0367)\n",
            "return\n",
            "tensor(-2.3659)\n",
            "return\n",
            "tensor(-2.3659)\n",
            "return\n",
            "tensor(0.6925)\n",
            "return\n",
            "tensor(0.6925)\n",
            "return\n",
            "tensor(0.6042)\n",
            "return\n",
            "tensor(0.6042)\n",
            "return\n",
            "tensor(0.6042)\n",
            "return\n",
            "tensor(0.6042)\n",
            "return\n",
            "tensor(-0.2505)\n",
            "return\n",
            "tensor(-0.2505)\n",
            "return\n",
            "tensor(-1.1119)\n",
            "return\n",
            "tensor(-1.1119)\n",
            "loss\n",
            "tensor(-0.0476, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1340, 0.2448, 0.1951, 0.1263, 0.0851, 0.2147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.2384, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-2.6732)\n",
            "return\n",
            "tensor(-2.6732)\n",
            "return\n",
            "tensor(0.0621)\n",
            "return\n",
            "tensor(0.0621)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(0.5691)\n",
            "return\n",
            "tensor(0.5691)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(-0.5226)\n",
            "return\n",
            "tensor(-0.5226)\n",
            "return\n",
            "tensor(0.7052)\n",
            "return\n",
            "tensor(0.7052)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(0.5399)\n",
            "return\n",
            "tensor(-0.3003)\n",
            "return\n",
            "tensor(-0.3003)\n",
            "loss\n",
            "tensor(-0.2340, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1344, 0.2447, 0.1941, 0.1251, 0.0836, 0.2181],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.1207, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-2.8182)\n",
            "return\n",
            "tensor(-2.8182)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(-0.2300)\n",
            "return\n",
            "tensor(-0.2300)\n",
            "return\n",
            "tensor(0.0279)\n",
            "return\n",
            "tensor(0.0279)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.2466)\n",
            "return\n",
            "tensor(0.2466)\n",
            "return\n",
            "tensor(0.2306)\n",
            "return\n",
            "tensor(0.2306)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.5781)\n",
            "return\n",
            "tensor(0.2306)\n",
            "return\n",
            "tensor(0.2306)\n",
            "loss\n",
            "tensor(0.0020, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1352, 0.2455, 0.1908, 0.1240, 0.0820, 0.2225],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.9000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.6290)\n",
            "return\n",
            "tensor(0.6290)\n",
            "return\n",
            "tensor(0.8992)\n",
            "return\n",
            "tensor(0.8992)\n",
            "return\n",
            "tensor(0.6290)\n",
            "return\n",
            "tensor(0.6290)\n",
            "return\n",
            "tensor(-1.7416)\n",
            "return\n",
            "tensor(-1.7416)\n",
            "return\n",
            "tensor(0.4253)\n",
            "return\n",
            "tensor(0.4253)\n",
            "return\n",
            "tensor(0.4714)\n",
            "return\n",
            "tensor(0.4714)\n",
            "return\n",
            "tensor(-0.7407)\n",
            "return\n",
            "tensor(-0.7407)\n",
            "return\n",
            "tensor(0.8992)\n",
            "return\n",
            "tensor(0.8992)\n",
            "return\n",
            "tensor(0.2708)\n",
            "return\n",
            "tensor(0.2708)\n",
            "return\n",
            "tensor(-1.7416)\n",
            "return\n",
            "tensor(-1.7416)\n",
            "loss\n",
            "tensor(-0.0326, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1362, 0.2477, 0.1871, 0.1231, 0.0802, 0.2257],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.1876, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-0.7418)\n",
            "return\n",
            "tensor(-0.7418)\n",
            "return\n",
            "tensor(0.4424)\n",
            "return\n",
            "tensor(0.4424)\n",
            "return\n",
            "tensor(0.9238)\n",
            "return\n",
            "tensor(0.9238)\n",
            "return\n",
            "tensor(-1.7584)\n",
            "return\n",
            "tensor(-1.7584)\n",
            "return\n",
            "tensor(0.6493)\n",
            "return\n",
            "tensor(0.6493)\n",
            "return\n",
            "tensor(0.4893)\n",
            "return\n",
            "tensor(0.4893)\n",
            "return\n",
            "tensor(0.6493)\n",
            "return\n",
            "tensor(0.6493)\n",
            "return\n",
            "tensor(-1.7584)\n",
            "return\n",
            "tensor(-1.7584)\n",
            "return\n",
            "tensor(0.6620)\n",
            "return\n",
            "tensor(0.6620)\n",
            "return\n",
            "tensor(0.4424)\n",
            "return\n",
            "tensor(0.4424)\n",
            "loss\n",
            "tensor(0.1798, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1381, 0.2477, 0.1852, 0.1231, 0.0795, 0.2264],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.0318, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.6398)\n",
            "return\n",
            "tensor(0.6398)\n",
            "return\n",
            "tensor(0.2871)\n",
            "return\n",
            "tensor(0.2871)\n",
            "return\n",
            "tensor(0.3033)\n",
            "return\n",
            "tensor(0.3033)\n",
            "return\n",
            "tensor(0.2871)\n",
            "return\n",
            "tensor(0.2871)\n",
            "return\n",
            "tensor(-2.8068)\n",
            "return\n",
            "tensor(-2.8068)\n",
            "return\n",
            "tensor(-0.1129)\n",
            "return\n",
            "tensor(-0.1129)\n",
            "return\n",
            "tensor(0.3033)\n",
            "return\n",
            "tensor(0.3033)\n",
            "return\n",
            "tensor(-0.1804)\n",
            "return\n",
            "tensor(-0.1804)\n",
            "return\n",
            "tensor(0.6398)\n",
            "return\n",
            "tensor(0.6398)\n",
            "return\n",
            "tensor(0.6398)\n",
            "return\n",
            "tensor(0.6398)\n",
            "loss\n",
            "tensor(-0.2745, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1391, 0.2499, 0.1836, 0.1207, 0.0778, 0.2288],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.1876, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.8771)\n",
            "return\n",
            "tensor(0.8771)\n",
            "return\n",
            "tensor(0.4272)\n",
            "return\n",
            "tensor(0.4272)\n",
            "return\n",
            "tensor(0.3788)\n",
            "return\n",
            "tensor(0.3788)\n",
            "return\n",
            "tensor(-1.8995)\n",
            "return\n",
            "tensor(-1.8995)\n",
            "return\n",
            "tensor(0.2164)\n",
            "return\n",
            "tensor(0.2164)\n",
            "return\n",
            "tensor(-1.8995)\n",
            "return\n",
            "tensor(-1.8995)\n",
            "return\n",
            "tensor(0.8771)\n",
            "return\n",
            "tensor(0.8771)\n",
            "return\n",
            "tensor(0.4272)\n",
            "return\n",
            "tensor(0.4272)\n",
            "return\n",
            "tensor(0.2164)\n",
            "return\n",
            "tensor(0.2164)\n",
            "return\n",
            "tensor(0.3788)\n",
            "return\n",
            "tensor(0.3788)\n",
            "loss\n",
            "tensor(-0.0016, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1407, 0.2500, 0.1829, 0.1179, 0.0762, 0.2323],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.9000, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.4594)\n",
            "return\n",
            "tensor(0.4594)\n",
            "return\n",
            "tensor(0.8319)\n",
            "return\n",
            "tensor(0.8319)\n",
            "return\n",
            "tensor(0.4594)\n",
            "return\n",
            "tensor(0.4594)\n",
            "return\n",
            "tensor(0.3298)\n",
            "return\n",
            "tensor(0.3298)\n",
            "return\n",
            "tensor(-1.4674)\n",
            "return\n",
            "tensor(-1.4674)\n",
            "return\n",
            "tensor(0.8319)\n",
            "return\n",
            "tensor(0.8319)\n",
            "return\n",
            "tensor(0.8319)\n",
            "return\n",
            "tensor(0.8319)\n",
            "return\n",
            "tensor(0.6579)\n",
            "return\n",
            "tensor(0.6579)\n",
            "return\n",
            "tensor(-1.4674)\n",
            "return\n",
            "tensor(-1.4674)\n",
            "return\n",
            "tensor(-1.4674)\n",
            "return\n",
            "tensor(-1.4674)\n",
            "loss\n",
            "tensor(0.1310, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1430, 0.2474, 0.1812, 0.1159, 0.0753, 0.2372],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.2384, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.3644)\n",
            "return\n",
            "tensor(0.3644)\n",
            "return\n",
            "tensor(0.5245)\n",
            "return\n",
            "tensor(0.5245)\n",
            "return\n",
            "tensor(-1.8840)\n",
            "return\n",
            "tensor(-1.8840)\n",
            "return\n",
            "tensor(0.7991)\n",
            "return\n",
            "tensor(0.7991)\n",
            "return\n",
            "tensor(0.7991)\n",
            "return\n",
            "tensor(0.7991)\n",
            "return\n",
            "tensor(0.7991)\n",
            "return\n",
            "tensor(0.7991)\n",
            "return\n",
            "tensor(0.1606)\n",
            "return\n",
            "tensor(0.1606)\n",
            "return\n",
            "tensor(0.1606)\n",
            "return\n",
            "tensor(0.1606)\n",
            "return\n",
            "tensor(-1.8840)\n",
            "return\n",
            "tensor(-1.8840)\n",
            "return\n",
            "tensor(0.1606)\n",
            "return\n",
            "tensor(0.1606)\n",
            "loss\n",
            "tensor(-0.1377, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1435, 0.2471, 0.1801, 0.1137, 0.0739, 0.2417],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.0318, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.2509)\n",
            "return\n",
            "tensor(0.2509)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(0.8329)\n",
            "return\n",
            "tensor(0.8329)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(0.4653)\n",
            "return\n",
            "tensor(0.4653)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(-2.7595)\n",
            "return\n",
            "tensor(-2.7595)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(-0.0219)\n",
            "return\n",
            "tensor(0.4653)\n",
            "return\n",
            "tensor(0.4653)\n",
            "return\n",
            "tensor(0.8329)\n",
            "return\n",
            "tensor(0.8329)\n",
            "loss\n",
            "tensor(-0.5623, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1433, 0.2497, 0.1788, 0.1099, 0.0690, 0.2494],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.1725, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(-1.6415)\n",
            "return\n",
            "tensor(-1.6415)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.6143)\n",
            "return\n",
            "tensor(0.6143)\n",
            "return\n",
            "tensor(0.6143)\n",
            "return\n",
            "tensor(0.6143)\n",
            "return\n",
            "tensor(-1.6415)\n",
            "return\n",
            "tensor(-1.6415)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(0.2436)\n",
            "return\n",
            "tensor(1.6659)\n",
            "return\n",
            "tensor(1.6659)\n",
            "return\n",
            "tensor(-0.5859)\n",
            "return\n",
            "tensor(-0.5859)\n",
            "loss\n",
            "tensor(-0.1284, grad_fn=<DivBackward0>)\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(0)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(4)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 4), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(2)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(5)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 2), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "for loop newdegree counter\n",
            "0\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(1)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 3), (5, 1), (6, 1)]\n",
            "for loop newdegree counter\n",
            "1\n",
            "softmax\n",
            "tensor([0.1429, 0.2516, 0.1774, 0.1052, 0.0640, 0.2588],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "explainer output\n",
            "tensor(3)\n",
            "degree of G while leaving for loop\n",
            "[(0, 1), (1, 2), (2, 2), (3, 3), (4, 3), (5, 1), (6, 2)]\n",
            "reward\n",
            "2\n",
            "reward\n",
            "2\n",
            "[0, tensor(-0.2384, grad_fn=<SubBackward0>)]\n",
            "DiscountedReturns\n",
            "20\n",
            "Advantage Function\n",
            "20\n",
            "return\n",
            "tensor(0.4688)\n",
            "return\n",
            "tensor(0.4688)\n",
            "return\n",
            "tensor(0.8580)\n",
            "return\n",
            "tensor(0.8580)\n",
            "return\n",
            "tensor(1.0394)\n",
            "return\n",
            "tensor(1.0394)\n",
            "return\n",
            "tensor(0.6509)\n",
            "return\n",
            "tensor(0.6509)\n",
            "return\n",
            "tensor(-1.3586)\n",
            "return\n",
            "tensor(-1.3586)\n",
            "return\n",
            "tensor(-1.3586)\n",
            "return\n",
            "tensor(-1.3586)\n",
            "return\n",
            "tensor(-0.4497)\n",
            "return\n",
            "tensor(-0.4497)\n",
            "return\n",
            "tensor(-1.3586)\n",
            "return\n",
            "tensor(-1.3586)\n",
            "return\n",
            "tensor(1.0394)\n",
            "return\n",
            "tensor(1.0394)\n",
            "return\n",
            "tensor(0.4688)\n",
            "return\n",
            "tensor(0.4688)\n",
            "loss\n",
            "tensor(0.0881, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(4)\n",
        "task='node'\n",
        "\n",
        "temp=1\n",
        "tesdat=[]\n",
        "\n",
        "discount=1\n",
        "  \n",
        "\n",
        "classes=0 # set to 0 if node is part of a cycle else set to 1\n",
        "loss1=nn.BCELoss()\n",
        "newdegree=2\n",
        "rollout=3\n",
        "numepisodes=10\n",
        "\n",
        "for j in range(len(dataset)):\n",
        "  num_gennodes=graphlen[j]\n",
        "  explainer=GCNPolicy()\n",
        "  opt = torch.optim.Adam(explainer.parameters(), lr=1e-2)\n",
        "\n",
        "  #explainer.train()\n",
        "  \n",
        "  for epoch in range(50):\n",
        "      data=dataset[j]\n",
        "      geph=nx.Graph()\n",
        "      G=nx.Graph()\n",
        "      geph=graphlist[j]\n",
        "      G.add_nodes_from(geph)\n",
        "      G.add_edges_from(geph.edges)\n",
        "      #newdegree=2\n",
        "      \n",
        "      label=data.y\n",
        "      newmask=torch.ones(num_gennodes-1)\n",
        "      Actions=[]\n",
        "      Rewards=[]\n",
        "      States=[]\n",
        "      DiscountedReturns=[]\n",
        "\n",
        "      loss=0\n",
        "      \n",
        "      #print(label)\n",
        "      for episodes in range(numepisodes):\n",
        "        data=dataset[j]\n",
        "        geph=nx.Graph()\n",
        "        G=nx.Graph()\n",
        "        geph=graphlist[j]\n",
        "        G.add_nodes_from(geph)\n",
        "        G.add_edges_from(geph.edges)\n",
        "      #newdegree=2\n",
        "      \n",
        "        label=data.y\n",
        "        Rewards=[]\n",
        "        newmask=torch.ones(num_gennodes-1)\n",
        "\n",
        "        for k in range(newdegree):\n",
        "          #plt.figure()\n",
        "          #nx.draw_networkx(G, node_size=150,with_labels=True, node_color='pink')\n",
        "        \n",
        "          rollmask=torch.ones(num_gennodes-1)\n",
        "          print(\"for loop newdegree counter\")\n",
        "          print(k)\n",
        "          output=explainer(data)\n",
        "          States.append(data)\n",
        "          print(\"softmax\")\n",
        "        \n",
        "          print(output)\n",
        "          #o=output.numpy()\n",
        "          #o=np.multiply(o,newmask)\n",
        "          #target=output*newmask\n",
        "          #output=output*newmask\n",
        "          #print(output)\n",
        "          m = Categorical(output)\n",
        "          #print(m)\n",
        "          action = m.sample()\n",
        "          print(\"explainer output\")\n",
        "          print(action)\n",
        "          Actions.append(action)\n",
        "          a=action.item()\n",
        "          #output=output*newmask\n",
        "\n",
        "          rollmask[a]=0\n",
        "          G.add_edge(a,num_gennodes-1)\n",
        "          deg1=G.degree()\n",
        "          #print(\"degree of G while entering for loop\")\n",
        "          #print(deg1)\n",
        "          reward=0\n",
        "          #for r in range(rollout):\n",
        "            #V=nx.Graph()\n",
        "            #V.add_nodes_from(G)\n",
        "            #V.add_edges_from(G.edges)\n",
        "            #degroll=V.degree()\n",
        "            #print(\"degree of V before rollout\")\n",
        "            #print(degroll)\n",
        "            #degroll=list(degroll)\n",
        "            #degroll.sort(key=takeFirst)\n",
        "            #print(degroll)\n",
        "            #degroll=[degroll[i][1] for i in range(num_gennodes)]\n",
        "            #degcounter=degroll[num_gennodes-1]\n",
        "            #degroll=torch.FloatTensor(degroll)\n",
        "            #degroll=torch.reshape(deg,(num_gennodes,1))\n",
        "            #rolldata=pyg_utils.from_networkx(V)\n",
        "            #rolldata.x=degroll\n",
        "            #plt.figure()\n",
        "            #nx.draw_networkx(V, node_size=150,with_labels=True, node_color='white')\n",
        "          \n",
        "            #if(k!=newdegree-1):\n",
        "              #rolloutput=explainer(rolldata)\n",
        "              #rolloutput=rolloutput*rollmask\n",
        "              #rollm=Categorical(rolloutput)\n",
        "              #rollaction=rollm.sample()\n",
        "              #rollaction=rollaction.item()\n",
        "              #print(\"rollout action\")\n",
        "              #print(rollaction)\n",
        "              #V.add_edge(rollaction,num_gennodes-1)\n",
        "            #degroll=V.degree()\n",
        "            #degroll=list(degroll)\n",
        "            #degroll.sort(key=takeFirst)\n",
        "            #print(\"degree of V after rollout\")\n",
        "            #print(degroll)\n",
        "            #degroll=[degroll[i][1] for i in range(num_gennodes)]\n",
        "            #degroll=torch.FloatTensor(degroll)\n",
        "            #degroll=torch.reshape(deg,(num_gennodes,1))\n",
        "            #rolldata=pyg_utils.from_networkx(V)\n",
        "            #rolldata.x=degroll\n",
        "            #rollemb,classifierreward,rollpred=model(rolldata)\n",
        "            #pred=rollpred.argmax(dim=1)\n",
        "            #reward+=(classifierreward[num_gennodes-1][classes]-0.5)*100\n",
        "            #plt.figure()\n",
        "            #nx.draw_networkx(V, node_size=150,with_labels=True, node_color='yellow')\n",
        "            #pred.eq(label).sum().item()+\n",
        "            #pred.eq(label).sum().item()\n",
        "          #reward=reward/rollout\n",
        "         \n",
        "        \n",
        "          \n",
        "          deg=G.degree()\n",
        "          deg=list(deg)\n",
        "          deg.sort(key=takeFirst)\n",
        "          print(\"degree of G while leaving for loop\")\n",
        "          print(deg)\n",
        "          deg=[deg[i][1] for i in range(num_gennodes)]\n",
        "          #print(deg)\n",
        "          deg=torch.FloatTensor(deg)\n",
        "          deg=torch.reshape(deg,(num_gennodes,1))\n",
        "          newdata=pyg_utils.from_networkx(G)\n",
        "          newdata.x=deg\n",
        "          newdata.y=data.y\n",
        "          #print(data.x)\n",
        "          data=newdata\n",
        "          #print(data.x)\n",
        "          if(k!=newdegree-1):\n",
        "            reward=0\n",
        "          else:\n",
        "            rollemb,classifierreward,rollpred=model(data)\n",
        "            #print(classifierreward)\n",
        "            reward=(classifierreward[num_gennodes-1][classes]-0.9)\n",
        "          Rewards.append(reward)\n",
        "          #emb1,rew1,pred1=model(data)\n",
        "          #pred=pred1.argmax(dim=1)\n",
        "          #reward2=(rew1[num_gennodes-1][classes]-0.5)\n",
        "          #reward2=0\n",
        "          #pred.eq(label).sum().item()+\n",
        "          #reward=reward+reward2\n",
        "          #opt.zero_grad()\n",
        "          #loss+=-m.log_prob(action)*(reward)\n",
        "          #if(epoch%10==0):\n",
        "          #print(\"loss\")\n",
        "          #print(loss)\n",
        "     #compute discounted returns\n",
        "      #DiscountedReturns=[]\n",
        "        for t in range(len(Rewards)):\n",
        "          print(\"reward\")\n",
        "          print(len(Rewards))\n",
        "          sum=0.0\n",
        "          for v,r in enumerate(Rewards[t:]):\n",
        "            sum+=r\n",
        "          DiscountedReturns.append(sum)\n",
        "      #print(\"DiscountedReturns\")\n",
        "      #print(DiscountedReturns)\n",
        "      print(Rewards)\n",
        "      #for x in range(1):\n",
        "        \n",
        "        #print(\"enter loss calculation\")\n",
        "        #States=torch.FloatTensor(States)\n",
        "        #probs=explainer(States)\n",
        "        #dist=torch.distributions.Categorical(probs=probs)\n",
        "        #log_prob = dist.log_prob(Actions)\n",
        "        #loss=-(log_prob*DiscountedReturns).sum()/num_episodes\n",
        "        #opt.zero_grad()\n",
        "        #opt.step()\n",
        "\n",
        "\n",
        "      loss=0\n",
        "      print(\"DiscountedReturns\")\n",
        "      print(len(DiscountedReturns))\n",
        "      DiscountedReturns=torch.tensor(DiscountedReturns)\n",
        "      DiscountedReturns=(DiscountedReturns-DiscountedReturns.mean())/(DiscountedReturns.std()+1e-9)\n",
        "      #compute baseline for advantage functions\n",
        "      b1=0\n",
        "      b2=0\n",
        "      ele=0\n",
        "      ele1=0\n",
        "      Advantage=[]\n",
        "      countstep=0\n",
        "      while(ele<len(DiscountedReturns)):\n",
        "        if(ele%2==0):\n",
        "          b1+=DiscountedReturns[ele]\n",
        "          countstep+=1\n",
        "        else:\n",
        "          b2+=DiscountedReturns[ele]\n",
        "        ele+=1\n",
        "      b1=b1/countstep\n",
        "      b2=b2/countstep\n",
        "      while(ele1<len(DiscountedReturns)):\n",
        "        if(ele1%2==0):\n",
        "         Advantage.append(DiscountedReturns[ele1]-b1)\n",
        "        else:\n",
        "          Advantage.append(DiscountedReturns[ele1]-b2)\n",
        "        ele1+=1\n",
        "      print(\"Advantage Function\")\n",
        "      print(len(Advantage))\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      for State, Action, R in zip(States, Actions, DiscountedReturns):\n",
        "        print(\"return\")\n",
        "        print(R)\n",
        "        probs = explainer(State)\n",
        "        dist = torch.distributions.Categorical(probs=probs)    \n",
        "        log_prob = dist.log_prob(Action)\n",
        "        \n",
        "        loss+= - log_prob*R\n",
        "        \n",
        "      newloss=loss/numepisodes\n",
        "      print(\"loss\")\n",
        "      print(newloss)\n",
        "      opt.zero_grad()\n",
        "      newloss.backward()\n",
        "      opt.step()\n",
        "      \n",
        "  color_map = []\n",
        "  for node in G:\n",
        "      if node < num_gennodes-1:\n",
        "          color_map.append('red')\n",
        "      else: \n",
        "          color_map.append('green')\n",
        "  tesdat.append(data)\n",
        "  plt.figure(j+1,figsize=(3,3))\n",
        "  #plt.title(\"plot of figure %i\"%classes)\n",
        "  nx.draw_networkx(G, node_size=70,with_labels=False, node_color=color_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUFd93lg75ak"
      },
      "source": [
        "Evaluation of the class score after neighborhoods are generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x7JyPuPSwba",
        "outputId": "99e439d7-7bed-4e1c-ec4b-7af8ffd86076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5835, 0.4165], grad_fn=<SelectBackward0>)\n",
            "tensor([0, 1, 1, 0])\n",
            "tensor([0.9327, 0.0673], grad_fn=<SelectBackward0>)\n",
            "tensor([1, 0, 0, 0, 0])\n",
            "tensor([0.7392, 0.2608], grad_fn=<SelectBackward0>)\n",
            "tensor([1, 1, 1, 0, 1, 0])\n",
            "tensor([0.6616, 0.3384], grad_fn=<SelectBackward0>)\n",
            "tensor([1, 0, 0, 1, 1, 1, 0])\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(tesdat)):\n",
        "  rolldata=tesdat[i]\n",
        "  rollemb,classifierreward,rollpred=model(rolldata)\n",
        "  #print(rollpred)\n",
        "  print(classifierreward[-1])\n",
        "  print(rollpred.argmax(dim=1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Node Classifier Explainer Synthetic",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
